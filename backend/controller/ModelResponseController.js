import { GoogleGenerativeAI } from "@google/generative-ai";
import { pool } from "../Database.js";
import { generateContext } from '../ContextGenerator.js';
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-2.0-flash" });
import dotenv from 'dotenv';
import { Redisclient } from "../caching/RedisConfig.js";
dotenv.config();
import { getDeepMemoryForUser } from "../ContextGenerator.js";
import { getMatchingMessages } from "./chatController.js";

// Generates AI response using merged summaries and recent messages
//based on matching cosine values
export const GetAIResponse = async (message, sender_id, roomName, userIsPaid, embeddings) => {
  try {
    if (!message || !roomName || !embeddings) {
      return { error: "AI requires input to respond" };
    }

    const MatchingMessagesFromQdrnt = await getMatchingMessages(message, sender_id, embeddings);

    // Get context from summaries/recent chats (ContextProvider should return an array of messages)
    const combinedContext = await ContextProvider(sender_id, userIsPaid);

    let deep_memory_messages = []; // Use a more descriptive name
    if (await getDeepMemoryForUser(sender_id) !== null) {
      deep_memory_messages = await getDeepMemoryForUser(sender_id); // Ensure this also returns an array of message objects
    }

    // IMPORTANT: Construct the full conversation history array correctly
    const conversationHistory = [
      { role: "model", parts: [{ text: process.env.SYSTEM_PROMPT }] }, // The initial system prompt (Alice's persona)
      ...deep_memory_messages, // Your long-term memory
      ...combinedContext,     // Summaries and potentially recent chats (from ContextProvider)
      ...MatchingMessagesFromQdrnt // Relevant messages from Qdrant
    ];

    // Add the current user message at the very end of the history
    conversationHistory.push({ role: "user", parts: [{ text: message }] });

    // console.log("Final Conversation History:", JSON.stringify(conversationHistory, null, 2)); // For debugging

    const result = await model.generateContent({
      contents: conversationHistory, // Pass the combined conversation history here
      generationConfig: {
        temperature: 0.8,
        topP: 0.95,
        topK: 40,
        maxOutputTokens: 600,
      }
    });

    const responseText = result.response.text();
    if (!responseText) {
      return "I am having some issues right now, can we talk later, I am really sorry for this issue, thanks for understanding me ";
    }

    return { sender: sender_id, response: responseText };
  } catch (error) {
    console.error("GetAIResponse error:", error);
    throw error;
  }
};




// Provides merged context: summaries first, then recent messages
async function ContextProvider(sender_id, userIsPaid) {
  if (!sender_id) return [];
  try {
    const AI_ID = 0;
    const roomName = [sender_id, AI_ID].sort((a, b) => a - b).join("_");
    const RedisKey = `RoomInfo:${roomName}:roomHistory`;
    const hasHistoryCached = await Redisclient.get(RedisKey);

    if (hasHistoryCached) {
      const parsedHistory = JSON.parse(hasHistoryCached);
      // Ensure these are proper message objects if they contain user/model turns
      return parsedHistory
        .filter(msg => msg.message && msg.message.trim().length > 0)
        .map(msg => ({
          role: msg.sender_type === "user" ? "user" : "model", // Assuming msg.sender_type exists in cached data
          parts: [{ text: msg.message }]
        }));
    }


    const contextQuery = `
      SELECT summary
      FROM chat_context
      WHERE room_name = $1
      ORDER BY generated_at DESC
      LIMIT $2
    `;
    const { rows: summaryRows } = await pool.query(contextQuery, [roomName, userIsPaid === false ? 5 : 10]);
    const formattedSummaries = summaryRows.map(r => {
      if (!r.summary) return null;
      return {
        role: "model", // Assuming summaries are generated by the model/system
        parts: [{ text: r.summary }]
      };
    }).filter(Boolean);

    // If you intend to use recent chats, uncomment and ensure 'formattedRecent' is properly defined.
    // Right now, this function ONLY returns summaries if Redis cache is missed.
    return [
      ...formattedSummaries,
      // ...formattedRecent, // If you want to include these, uncomment this and the related SQL query above
    ];
  } catch (error) {
    console.error("ContextProvider error:", error);
    return [];
  }
}


